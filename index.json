[{"content":"Introduction In today\u0026rsquo;s cloud-centric era, optimizing costs is a crucial aspect of managing your infrastructure. AWS (Amazon Web Services) offers a wide range of services and features that can help you reduce your cloud expenses. One such effective method is implementing an efficient tagging strategy. By leveraging AWS tagging capabilities, you can gain better visibility, control, and cost allocation across your resources. In this blog post, we will explore the importance of an AWS tagging strategy for cost saving and optimization.\nWhat are AWS tags? AWS tags are metadata labels consisting of key-value pairs that you can assign to your AWS resources. These tags act as labels or markers, allowing you to categorize and organize your resources in a logical manner. For example, you can assign tags to identify resources by their purpose, environment (e.g., production, development), department, project, or any other custom attribute that suits your organization\u0026rsquo;s needs.\nWhy is a tagging strategy important? Cost Allocation: An effective tagging strategy enables you to accurately allocate costs across your organization. By tagging resources with relevant attributes, you can easily identify the costs associated with specific projects, departments, or teams. This visibility helps in budgeting, forecasting, and optimizing spending.\nResource Management: Tags provide a structured way to organize and manage your resources. With a well-defined tagging strategy, you can quickly locate, filter, and group resources based on specific criteria. This simplifies tasks like resource tracking, access control, and lifecycle management.\nOptimization Opportunities: Tags allow you to gain insights into resource utilization patterns. By analyzing resource usage based on tags, you can identify opportunities for cost optimization. For instance, you might discover that certain instances are consistently underutilized, enabling you to downsize or terminate them, thus reducing costs.\nAutomation and Policy Enforcement: AWS provides powerful tools like AWS Config and AWS Resource Groups, which can be configured to trigger automated actions based on tags. This capability enables you to enforce cost-saving policies automatically. For example, you can set up a policy to terminate instances with a specific tag if they are idle for a certain period, preventing unnecessary costs.\nBest practices for an effective tagging strategy Consistent and Standardized Approach: Establish clear guidelines for tag key names and values. Consistency ensures that tags are meaningful and useful across your organization. Consider using a standardized taxonomy that aligns with your organizational structure and objectives.\nTagging Resources: Tag resources as soon as they are created, and enforce tagging policies to avoid resource proliferation without tags. Use AWS Tag Editor or APIs to tag resources in bulk or programmatically.\nResource Categorization: Determine the most relevant dimensions for your tags. Consider factors such as cost allocation requirements, resource ownership, environment, project, and application-specific attributes. This helps in creating a granular view of your resource usage and costs.\nAutomation: Leverage AWS tools like AWS Systems Manager, AWS Config, and AWS Lambda to automate tagging processes. Implementing automation ensures consistent application of tags and reduces the chances of manual errors.\nRegular Review: Perform periodic reviews of your tagging strategy to ensure its effectiveness and relevance. Tagging requirements may evolve over time, so it\u0026rsquo;s crucial to adapt and optimize your strategy accordingly.\nConclusion Implementing a well-structured AWS tagging strategy is a powerful approach to optimize costs and gain control over your cloud resources. By effectively categorizing and organizing your resources through tags, you can allocate costs accurately, manage resources efficiently, and identify opportunities for optimization. Remember to define clear guidelines, automate where possible, and regularly review and refine your tagging strategy. With a well-executed tagging strategy, you can unlock the full potential of AWS cost savings and ensure optimal resource utilization in your cloud environment\n","permalink":"https://omers.github.io/docs/cloud-management/aws-tagging-stratery-for-cost-saving/","summary":"Introduction In today\u0026rsquo;s cloud-centric era, optimizing costs is a crucial aspect of managing your infrastructure. AWS (Amazon Web Services) offers a wide range of services and features that can help you reduce your cloud expenses. One such effective method is implementing an efficient tagging strategy. By leveraging AWS tagging capabilities, you can gain better visibility, control, and cost allocation across your resources. In this blog post, we will explore the importance of an AWS tagging strategy for cost saving and optimization.","title":"Aws Tagging Stratery for Cost Saving"},{"content":"Cost Saving with AWS ECS and Fargate Spot Instances Are you looking for ways to optimize your infrastructure costs while maintaining the scalability and reliability of your applications? AWS ECS (Elastic Container Service) with Fargate Spot instances might be the solution you\u0026rsquo;re searching for. In this blog post, we will explore how you can leverage Fargate Spot instances to achieve significant cost savings without compromising on performance or availability.\nUnderstanding ECS and Fargate Before diving into Fargate Spot instances, let\u0026rsquo;s quickly recap what AWS ECS and Fargate are. AWS ECS is a fully managed container orchestration service that allows you to run and scale Docker containers on AWS. It provides a flexible and scalable platform for deploying and managing containerized applications.\nFargate, on the other hand, is a compute engine for AWS ECS that allows you to run containers without the need to manage the underlying infrastructure. It abstracts away the EC2 instances, enabling you to focus solely on running your containers and leaving the infrastructure management to AWS.\nIntroducing Fargate Spot Instances Fargate Spot instances are a cost-effective option for running containers with AWS Fargate. These instances provide the same capabilities as regular Fargate instances but at a significantly lower cost. The catch is that Fargate Spot instances can be interrupted by AWS with a two-minute notification when EC2 capacity is needed by higher-priority workloads. However, for many applications, this temporary interruption is acceptable and can be easily handled with proper architectural design.\nLeveraging Fargate Spot Instances for Cost Savings Using Fargate Spot instances can result in substantial cost savings for your containerized workloads. Here are a few strategies to maximize your savings:\n1. Identify Spot-Compatible Workloads Certain workloads are more suitable for running on Fargate Spot instances than others. Applications with built-in fault tolerance, such as stateless microservices or batch processing jobs, can handle interruptions without significant impact. Identify these workloads in your environment and prioritize them for migration to Fargate Spot.\n2. Fine-Tune Spot Allocation Strategy AWS allows you to define the maximum price you\u0026rsquo;re willing to pay for Fargate Spot instances. By setting a sensible maximum price, you can strike a balance between cost savings and availability. Experiment with different maximum prices to find the sweet spot for your workloads.\n3. Implement Application Resilience To handle Fargate Spot interruptions gracefully, design your applications to be resilient. Use features like auto-scaling and task placement strategies to distribute your containers across multiple availability zones. By ensuring redundancy, your applications can continue running seamlessly even if some instances are interrupted.\n4. Leverage Spot Instance Pools Spot instance pools are groups of instances with similar pricing and interruption behavior. AWS Fargate manages these pools and automatically allocates Spot instances to your tasks. By leveraging spot instance pools, you can increase the chances of receiving Spot instances consistently while maintaining a cost-efficient infrastructure.\n5. Monitor and Optimize Regularly monitor your Fargate Spot instances and their interruptions to gather insights into their behavior. AWS provides detailed metrics and logs to help you understand the interruption patterns and adjust your strategies accordingly. Fine-tune your Spot allocation strategy and task placement to optimize cost savings without compromising availability.\nConclusion AWS ECS with Fargate Spot instances offers an excellent opportunity to reduce your infrastructure costs without compromising on performance and scalability. By identifying suitable workloads, fine-tuning your allocation strategy, implementing application resilience, and leveraging spot instance pools, you can unlock substantial cost savings while still maintaining a reliable and highly available environment.\nAs always, it\u0026rsquo;s crucial to analyze your specific requirements, workload characteristics, and cost tolerance before adopting Fargate Spot instances\n","permalink":"https://omers.github.io/docs/cloud-management/aws-cost-saving-for-ecs-cluster/","summary":"Cost Saving with AWS ECS and Fargate Spot Instances Are you looking for ways to optimize your infrastructure costs while maintaining the scalability and reliability of your applications? AWS ECS (Elastic Container Service) with Fargate Spot instances might be the solution you\u0026rsquo;re searching for. In this blog post, we will explore how you can leverage Fargate Spot instances to achieve significant cost savings without compromising on performance or availability.\nUnderstanding ECS and Fargate Before diving into Fargate Spot instances, let\u0026rsquo;s quickly recap what AWS ECS and Fargate are.","title":"Aws Cost Saving for ECS Cluster"},{"content":"The Idea Elasticsearch and Grafana Loki are both log management systems, but they have different strengths and weaknesses. Here are some reasons why you might want to consider migrating from Elasticsearch to Grafana Loki:\nCost: Elasticsearch can be expensive, especially for larger deployments. Grafana Loki, on the other hand, is open source and free to use.\nSimplicity: Grafana Loki is designed to be simple and easy to use, with a focus on searching and visualizing logs. Elasticsearch, while powerful, can be more complex to set up and configure.\nScalability: Grafana Loki is designed to be highly scalable and performant, with a focus on horizontal scalability. Elasticsearch can also be scaled horizontally, but it requires more effort to set up and configure.\nKubernetes integration: Grafana Loki was designed with Kubernetes in mind, and integrates seamlessly with Kubernetes clusters. Elasticsearch can be integrated with Kubernetes, but it requires more effort and configuration.\nQuery language: Grafana Loki uses a query language that is similar to PromQL, which makes it easy to learn and use for users who are familiar with Prometheus. Elasticsearch uses its own query language, which can be more complex and difficult to learn.\nUltimately, the decision to migrate from Elasticsearch to Grafana Loki will depend on your specific needs and use case. However, if you are looking for a more cost-effective, simple, scalable, and Kubernetes-friendly log management system, Grafana Loki might be a good choice for you.\nStream 600Gb logs per day\nWhy Grafana Loki? Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost effective and easy to operate. It does not index the contents of the logs, but rather a set of labels for each log stream.\nLoki takes a unique approach by only indexing the metadata rather than the full text of the log lines.\nBusiness Value Better alerting mechanism Cost saving - Store on S3 Longer retention period Better integration with Grafana Cost Compute: 3 x c6i.4xlarge\t= 0.68 x 3 x 24 x 30 = $1468\nStorage: STANDARD TIER save for 90 days STANDARD_IA TIER - after 90 days move to STANDARD_IA -\u0026gt; After 180 days delete\nNetwork:\nSolution Architecture Ingesters: Ingesters are responsible for ingesting log data from producers. Queriers: Queriers are responsible for querying log data. Compactors: Compactors are responsible for compacting log data to reduce storage space. Rulers: Rulers are responsible for enforcing policies on log data. Index gateways: Index gateways are responsible for serving log data to clients. Read Nodes Write Nodes Backend Storage Data Retention The retention_deletes_enabled flag specifies whether or not Grafana Loki should delete old log data. The retention_period specifies the number of days that log data will be retained.\nOnce you have configured the Loki object store and table manager configurations, you can start collecting and storing log data in Grafana Loki.\ncompactor: working_directory: /data/retention shared_store: s3 compaction_interval: 10m retention_enabled: true retention_delete_delay: 2h retention_delete_worker_count: 150 chunk_store_config: max_look_back_period: 672h table_manager: retention_deletes_enabled: true retention_period: 7d Visualization Load balancer Write rules\nRead rules\nComponents Infrastrucutre as code ","permalink":"https://omers.github.io/docs/monitoring/building-loki-cluster-with-terraform/","summary":"The Idea Elasticsearch and Grafana Loki are both log management systems, but they have different strengths and weaknesses. Here are some reasons why you might want to consider migrating from Elasticsearch to Grafana Loki:\nCost: Elasticsearch can be expensive, especially for larger deployments. Grafana Loki, on the other hand, is open source and free to use.\nSimplicity: Grafana Loki is designed to be simple and easy to use, with a focus on searching and visualizing logs.","title":"Building Loki Cluster With Terraform"},{"content":"It is possible to manage Consul\u0026rsquo;s key-value store using version control tools like Git. One way to do this is to use a tool that synchronizes the key-value store with a Git repository. This allows you to track changes to the key-value store using Git and merge changes from multiple users using standard Git workflows.\nHere are the general steps for managing Consul\u0026rsquo;s key-value store using Git:\nInstall a tool that can synchronize the key-value store with a Git repository. There are several tools available for this purpose, such as consul-kv-sync and consul-sync.\nConfigure the synchronization tool to connect to your Consul server and the Git repository where you want to store the key-value data.\nUse the synchronization tool to export the key-value data from Consul to the Git repository. This will create a commit in the repository with the key-value data as a file.\nMake any changes to the key-value data that you want to track using Git. These changes can be made directly in the Git repository or through the Consul key-value store using the synchronization tool.\nUse the synchronization tool to import the changes from the Git repository back into Consul. This will update the key-value store with the changes made in Git.\nBy using a tool to synchronize Consul\u0026rsquo;s key-value store with a Git repository, you can track changes to the key-value store using Git and use standard Git workflows to merge changes from multiple users.\n","permalink":"https://omers.github.io/docs/cloud-management/manage-consul-kv-with-git/","summary":"It is possible to manage Consul\u0026rsquo;s key-value store using version control tools like Git. One way to do this is to use a tool that synchronizes the key-value store with a Git repository. This allows you to track changes to the key-value store using Git and merge changes from multiple users using standard Git workflows.\nHere are the general steps for managing Consul\u0026rsquo;s key-value store using Git:\nInstall a tool that can synchronize the key-value store with a Git repository.","title":"Manage Consul K/V With Git"},{"content":"Create ECR Repository with vulnerability scanning aws ecr create-repository --repository-name \u0026lt;MY_ECR_REPO_NAME\u0026gt; \\ --image-scanning-configuration scanOnPush=true --region us-east-2 ","permalink":"https://omers.github.io/docs/aws/setting-up-ecr/","summary":"Create ECR Repository with vulnerability scanning aws ecr create-repository --repository-name \u0026lt;MY_ECR_REPO_NAME\u0026gt; \\ --image-scanning-configuration scanOnPush=true --region us-east-2 ","title":"Setting Up AWS ECR service"},{"content":"Introduction In the dynamic landscape of Research and Development (R\u0026amp;D), organizations strive to optimize their productivity and achieve remarkable results. To accomplish this, it\u0026rsquo;s crucial to adopt data-driven practices and metrics that provide valuable insights into the development process. One such methodology gaining prominence is the use of DORA metrics, which stands for DevOps Research and Assessment.\nThis blog post will delve into the significance of DORA metrics and how they can be effectively utilized to enhance the productivity of an R\u0026amp;D group consisting of 70 individuals. By understanding the key metrics and implementing them in a thoughtful manner, organizations can foster a culture of continuous improvement and achieve outstanding outcomes.\nWhat is DORA? DORA is the acronym for the DevOps Research and Assessment group: they‚Äôve surveyed more than 50,000 technical professionals worldwide to better understand how the technical practices, cultural norms, and management approach affect organizational performance.\nThe four key metrics we measure Deployment Frequency ‚Äî How often an organization successfully releases to production Lead Time for Changes ‚Äî The amount of time it takes a commit to get into production Change Failure Rate ‚Äî The percentage of deployments causing a failure in production Time to Restore Service ‚Äî How long it takes an organization to recover from a failure in production How do we understand the metrics? Deployment Frequency - limiting amount of code going to * production at once (limited batch size) Lead Time for Changes - reducing amount of blockers for developers Time to Restore Service - improving speed of incident recovery Change Failure Rate - improving quality focus Following the rules of lean: Value is only released to production, once it leaves the factory floor (production deploy frequency) Optimize Work In Progress (lead time) Invest in SRE/automation (mean time to recover) Practice kaizen (change fail rate) Have efficient knowledge sharing and work allocation processes (time to onboard) Get into business To effectively utilize DORA metrics within an R\u0026amp;D group of 70 people, organizations should consider the following steps:\nEstablish a Baseline: Begin by measuring the current performance of the R\u0026amp;D group across the DORA metrics. This provides a starting point for improvement and helps set realistic goals. Define Objectives: Identify specific objectives that align with the organization\u0026rsquo;s overall goals and strategies. These objectives should be measurable, achievable, relevant, and time-bound (SMART goals). For example, aim to increase deployment frequency by 20% within the next quarter. Foster a Culture of Learning: Encourage a culture of continuous learning and improvement within the R\u0026amp;D group. Create opportunities for knowledge sharing, experimentation, and innovation. Emphasize the importance of feedback and celebrate successes. Monitor Progress: Regularly track and analyze the DORA metrics to evaluate the progress towards the defined objectives. Use visual dashboards and reports to make the data easily accessible and understandable for the entire team. Iterate and Adjust: Based on the insights gained from the D ","permalink":"https://omers.github.io/docs/career/dora-metrics/","summary":"Introduction In the dynamic landscape of Research and Development (R\u0026amp;D), organizations strive to optimize their productivity and achieve remarkable results. To accomplish this, it\u0026rsquo;s crucial to adopt data-driven practices and metrics that provide valuable insights into the development process. One such methodology gaining prominence is the use of DORA metrics, which stands for DevOps Research and Assessment.\nThis blog post will delve into the significance of DORA metrics and how they can be effectively utilized to enhance the productivity of an R\u0026amp;D group consisting of 70 individuals.","title":"DORA Metrics"},{"content":"About Terracognita TerraCognita allows you to migrate your current cloud infrastructure to infrastructure-as-code. It quickly and automatically creates Terraform from all of your manually-provisioned resources, allowing them to be easily replicated by you and your cloud provider.\nThe Challange For example, There is a security group that was created manually long time ago.\nNow, You need to start amanaging and audit any events and changes related to this Security gtoup.\nThe code terracognita aws -i aws_security_group \\ --aws-access-key \u0026lt;YOUR_ACCESS_KEY\u0026gt; \\ --aws-secret-access-key \u0026lt;YOUR_SECRET_ACCESS_KEY\u0026gt; \\ --tfstate terraform.tfstate --hcl sg.tf \\ --aws-default-region us-east-2 --target \u0026lt;RESOURCE_ID\u0026gt; terracognita aws \u0026ndash;target aws_security_group.sg-0cf9d7d81a21c43c3 -i aws_security_group \u0026ndash;tfstate terraform.tfstate \u0026ndash;hcl sg.tf \u0026ndash;aws-default-region us-east-2\n","permalink":"https://omers.github.io/docs/cloud-management/importing-cloud-resources-terracognita/","summary":"About Terracognita TerraCognita allows you to migrate your current cloud infrastructure to infrastructure-as-code. It quickly and automatically creates Terraform from all of your manually-provisioned resources, allowing them to be easily replicated by you and your cloud provider.\nThe Challange For example, There is a security group that was created manually long time ago.\nNow, You need to start amanaging and audit any events and changes related to this Security gtoup.","title":"Importing Cloud Resources Terracognita"},{"content":"re:invent 2022 recorded session This session provides architectural best practices, optimizations, and useful cheat codes that you can use to build secure, high-scale, and high-performance serverless applications and uses real customer scenarios to illustrate the benefits. ","permalink":"https://omers.github.io/docs/aws/best-practices-for-serverless-developers/","summary":"re:invent 2022 recorded session This session provides architectural best practices, optimizations, and useful cheat codes that you can use to build secure, high-scale, and high-performance serverless applications and uses real customer scenarios to illustrate the benefits. ","title":"Best practices for advanced serverless developers"},{"content":"Teleport has a strong RBAC system that allows to grant access to specific resources.\nIn order to setup your environment, You need to do some preperations:\nDefine what are the Groups in your organization Define which access each group needs, For example:\nDevelopers group needs access to Grafana and server Superset Decide how to label each server / resource, and assign the labels to each resource Deinfe the policy that grants the access to the resources kind: role version: v5 metadata: name: Developers description: Developers Team Role spec: allow: logins: [\u0026#39;admin\u0026#39;] node_labels: \u0026#39;type\u0026#39;: \u0026#39;Grafana\u0026#39; \u0026#39;type\u0026#39;: \u0026#39;Superset\u0026#39; ","permalink":"https://omers.github.io/docs/teleport/access-and-authorization/","summary":"Teleport has a strong RBAC system that allows to grant access to specific resources.\nIn order to setup your environment, You need to do some preperations:\nDefine what are the Groups in your organization Define which access each group needs, For example:\nDevelopers group needs access to Grafana and server Superset Decide how to label each server / resource, and assign the labels to each resource Deinfe the policy that grants the access to the resources kind: role version: v5 metadata: name: Developers description: Developers Team Role spec: allow: logins: [\u0026#39;admin\u0026#39;] node_labels: \u0026#39;type\u0026#39;: \u0026#39;Grafana\u0026#39; \u0026#39;type\u0026#39;: \u0026#39;Superset\u0026#39; ","title":"Teleport Access and Authorization"},{"content":"Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, reduced cost, and it drives innovative solutions within the healthcare sector.\nHowever, health data is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR), which aims to ensure patient\u0026rsquo;s privacy.\nAnonymization or removal of patient identifiable information, though the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns.\nIn this post, I\u0026rsquo;ll review a Python package created my Microsoft that helps to ensure sensitive data is properly managed and governed.\nIt provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more..\nAllow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions. Embrace extensibility and customizability to a specific business need. Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms. Install pip install presidio-analyzer pip install presidio-anonymizer python -m spacy download en_core_web_lg Sample Code from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine text=\u0026#34;\u0026#34;\u0026#34; PATIENT: JOHN SMITH DOB: 5/5/1955 FILE #: 12345 PHYSICIAN: REFERRING EXAM: MRI ABDOMEN WITH CONTRAST DATE: 1/1/2011 Evaluation of the neck reveals a somewhat heterogeneous but well defined lobulated mass within the superficial lobe of the anterior right parotid gland. The mass demonstrates a rounded focus of signal prolongation with enhancement. The mass measures approximately 0.8 x 0.9 x 0.7 CM (anterior-posterior by transverse by superiorinferior). This appears similar to that noted on the prior CT. The mass just abuts the retromandibular vein, patent and medial to the right parotid duct. The mass is most consistent with a benign pleomorphic adenoma. No leftsided parotid mass is seen. Right and left submandibular glands are unremarkable. The mucosal surfaces of the upper aerodigestive tract appear symmetric and unremarkable. The larynx is intact. The nasopharynx is symmetric without distinct lesion. The tongue and tongue base appear symmetric and unremarkable. The median raphe is midline. The thyroid gland appears symmetric without distinct nodule. No pathologically enlarged lymph nodes are found. The visualized lymph nodes demonstrate no central necrosis or extranodal extension. The right and left faucial tonsil is symmetric and unremarkable. The lingual tonsillar tissue appears symmetric and of normal volume. The posterior nasopharyngeal lymphoid tissue does not appear enlarged. Evaluation of the paranasal sinuses reveals no significant sinus inflammatory disease. No air-fluid levels are noted. The central skull base is intact. The central petrous temporal bones and mastoids remain clear. The visualized base of brain appears unremarkable. Cervical spondylosis is noted, most notable for a broad-based disc bulge and dorsal osteophytic ridge at the C5/6 level with a C6/7 level focal 2 mm central disc protrusion and dorsal osteophytic ridging, resulting in mild central spinal stenosis. Mild foraminal narrowing also evident bilaterally. \u0026#34;\u0026#34;\u0026#34; # Set up the engine, loads the NLP module (spaCy model by default) # and other PII recognizers analyzer = AnalyzerEngine() # Call analyzer to get results results = analyzer.analyze(text=text,language=\u0026#39;en\u0026#39;) print(results) # Analyzer results are passed to the AnonymizerEngine for anonymization anonymizer = AnonymizerEngine() anonymized_text = anonymizer.anonymize(text=text,analyzer_results=results) print(anonymized_text) The output text: PATIENT: \u0026lt;PERSON\u0026gt;: \u0026lt;DATE_TIME\u0026gt; FILE #: 12345 PHYSICIAN: REFERRING EXAM: MRI ABDOMEN WITH CONTRAST DATE: \u0026lt;DATE_TIME\u0026gt; Marked hydronephrosis and hydroureter are present in the right kidney (series 12 images \u0026lt;DATE_TIME\u0026gt;). Low signal intensity foci in the proximal right ureter (series 6 image 36) likely represents flow related artifact. Possible septations may be present in the distal right ureter (series 12 image 20). CT scan of the abdomen and pelvis with and without contrast is recommended to evaluate for possible stone or distal obstructing lesion. Findings are new since the previous examination. Decreased enhancement of the right kidney in comparison to the left during the arterial phase (series 15 image 35) may reflect a renal compromise. Stable mild \u0026lt;PERSON\u0026gt; is again noted in the left kidney. No mass is identified in the kidneys. No masses seen along the right ureter. Postoperative changes are seen from a distal pancreatectomy and cholecystectomy representing previous \u0026lt;PERSON\u0026gt; procedure. There is dilatation of the pancreatic duct in the body and tail (series 6 images 23-20). No recurrent mass is seen in the pancreas or anastomosis. There is mild prominence of the biliary ducts in the left hepatic lobe (series 7 image 20). No filling defect is seen within the common duct. \u0026lt;PERSON\u0026gt; and adrenal glands are unremarkable. No free fluid or lymphadenopathy seen. No bowel obstruction is identified. Anterior abdominal hernia is again noted containing small bowel without evidence of strangulation (series 7 image 33). There is marked S-shaped scoliosis of the thoracolumbar spine. No metastatic bone lesions are identified. items: [ {\u0026#39;start\u0026#39;: 1296, \u0026#39;end\u0026#39;: 1304, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 987, \u0026#39;end\u0026#39;: 995, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 764, \u0026#39;end\u0026#39;: 772, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 206, \u0026#39;end\u0026#39;: 217, \u0026#39;entity_type\u0026#39;: \u0026#39;DATE_TIME\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;DATE_TIME\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 105, \u0026#39;end\u0026#39;: 116, \u0026#39;entity_type\u0026#39;: \u0026#39;DATE_TIME\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;DATE_TIME\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 20, \u0026#39;end\u0026#39;: 31, \u0026#39;entity_type\u0026#39;: \u0026#39;DATE_TIME\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;DATE_TIME\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 10, \u0026#39;end\u0026#39;: 18, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;} ] ","permalink":"https://omers.github.io/docs/data/anonymization/","summary":"Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, reduced cost, and it drives innovative solutions within the healthcare sector.\nHowever, health data is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR), which aims to ensure patient\u0026rsquo;s privacy.\nAnonymization or removal of patient identifiable information, though the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns.","title":"Data Protection and Anonymization"},{"content":"","permalink":"https://omers.github.io/docs/cloud-management/cloud-resources-governance/","summary":"","title":"Cloud Resources Governance"},{"content":"During the lifecycle of a company, we are required to have an assets inventory.\nThe process of having an asset inventory contains few steps.\nCloudQuery As the tool that scrapes AWS and gather all the information, We will use CloudQuery\nTimescaleDB Apache Superset ","permalink":"https://omers.github.io/docs/cloud-management/cloud-resources-dashboards/","summary":"During the lifecycle of a company, we are required to have an assets inventory.\nThe process of having an asset inventory contains few steps.\nCloudQuery As the tool that scrapes AWS and gather all the information, We will use CloudQuery\nTimescaleDB Apache Superset ","title":"Cloud Resources Dashboards"},{"content":"","permalink":"https://omers.github.io/docs/monitoring/cloudquery/","summary":"","title":"Cloudquery"},{"content":"n8n is an open-source workflow automation tool that allows you to automate tasks across different applications and services. It provides a visual interface that allows you to create workflows by connecting different nodes to perform specific tasks. n8n is built on top of Node-RED, a popular open-source programming tool for wiring together hardware devices, APIs, and online services. It is designed to be easy to use and suitable for a wide range of use cases, including integration with CRM systems, data analysis, and automation of social media tasks.\n","permalink":"https://omers.github.io/docs/automation/n8n/","summary":"n8n is an open-source workflow automation tool that allows you to automate tasks across different applications and services. It provides a visual interface that allows you to create workflows by connecting different nodes to perform specific tasks. n8n is built on top of Node-RED, a popular open-source programming tool for wiring together hardware devices, APIs, and online services. It is designed to be easy to use and suitable for a wide range of use cases, including integration with CRM systems, data analysis, and automation of social media tasks.","title":"N8n"},{"content":"This is a one liner on how to create SSL Self signed certificate with OpenSSL\nopenssl req -new -newkey rsa:4096 -days 365 \\ -nodes -x509 -keyout server.key -out server.crt ","permalink":"https://omers.github.io/docs/automation/generate-self-signed-cert/","summary":"This is a one liner on how to create SSL Self signed certificate with OpenSSL\nopenssl req -new -newkey rsa:4096 -days 365 \\ -nodes -x509 -keyout server.key -out server.crt ","title":"OpenSSL Generate self signed certificate"},{"content":"Loki is a datastore optimized for efficiently holding log data.\nThe efficient indexing of log data distinguishes Loki from other logging systems. Unlike other logging systems, a Loki index is built from labels, leaving the original log message unindexed.\nThe Basic native tool to send data to Loki, is Promtail\nA sample config file for Loki might seen like the follows:\nserver: http_listen_port: 0 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: https://your.loki.cluster scrape_configs: - job_name: system static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/*.log - targets: - localhost labels: job: syslog __path__: /var/log/syslog ","permalink":"https://omers.github.io/docs/monitoring/loki/","summary":"Loki is a datastore optimized for efficiently holding log data.\nThe efficient indexing of log data distinguishes Loki from other logging systems. Unlike other logging systems, a Loki index is built from labels, leaving the original log message unindexed.\nThe Basic native tool to send data to Loki, is Promtail\nA sample config file for Loki might seen like the follows:\nserver: http_listen_port: 0 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: https://your.","title":"Loki Log based metrics a.k.a logs2metrics"},{"content":"Quantiles from histograms 90th percentile request latency over last 5 minutes, for every label dimension:\nhistogram_quantile(0.9, rate(api_duration_seconds_bucket[5m])) Rates of increase for counters Per-second rate of increase, averaged over last 5 minutes: rate(demo_api_request_duration_seconds_count[5m]) Per-second rate of increase, calculated over last two samples in a 1-minute time window irate(demo_api_request_duration_seconds_count[1m]) Absolute increase over last hour: increase(demo_api_request_duration_seconds_count[1h]) Aggregating over time Average within each series over a 5-minute period avg_over_time(go_goroutines[5m]) Get the maximum for each series over a one-day period max_over_time(process_resident_memory_bytes[1d]) Count the number of samples for each series over a 5-minute period count_over_time(process_resident_memory_bytes[5m]) ","permalink":"https://omers.github.io/docs/monitoring/prometheus/common-queries/","summary":"Quantiles from histograms 90th percentile request latency over last 5 minutes, for every label dimension:\nhistogram_quantile(0.9, rate(api_duration_seconds_bucket[5m])) Rates of increase for counters Per-second rate of increase, averaged over last 5 minutes: rate(demo_api_request_duration_seconds_count[5m]) Per-second rate of increase, calculated over last two samples in a 1-minute time window irate(demo_api_request_duration_seconds_count[1m]) Absolute increase over last hour: increase(demo_api_request_duration_seconds_count[1h]) Aggregating over time Average within each series over a 5-minute period avg_over_time(go_goroutines[5m]) Get the maximum for each series over a one-day period max_over_time(process_resident_memory_bytes[1d]) Count the number of samples for each series over a 5-minute period count_over_time(process_resident_memory_bytes[5m]) ","title":"Prometheus Common Queries"},{"content":"YOLOv5 üöÄ is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.\nThe project source code can be found Here\n","permalink":"https://omers.github.io/docs/ai/yolov/","summary":"YOLOv5 üöÄ is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.\nThe project source code can be found Here","title":"YOLO ‚Äî You only look once"},{"content":"Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP.\nA lot of the data that you could be analyzing is unstructured data and contains human-readable text. Before you can analyze that data programmatically, you first need to preprocess it. In this tutorial, you‚Äôll take your first look at the kinds of text preprocessing tasks you can do with NLTK so that you‚Äôll be ready to apply them in future projects. You‚Äôll also see how to do some basic text analysis and create visualizations.\nIf you‚Äôre familiar with the basics of using Python and would like to get your feet wet with some NLP, then you‚Äôve come to the right place.\nBy the end of this tutorial, you‚Äôll know how to:\nFind text to analyze Preprocess your text for analysis Analyze your text Create visualizations based on your analysis Let‚Äôs get Pythoning! ","permalink":"https://omers.github.io/docs/ai/python-nlp-libraries/","summary":"Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP.\nA lot of the data that you could be analyzing is unstructured data and contains human-readable text. Before you can analyze that data programmatically, you first need to preprocess it. In this tutorial, you‚Äôll take your first look at the kinds of text preprocessing tasks you can do with NLTK so that you‚Äôll be ready to apply them in future projects.","title":"Python Nlp Libraries"},{"content":"Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.\n","permalink":"https://omers.github.io/docs/ai/slearn/","summary":"Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.","title":"Python Scikit Learn"},{"content":"◊†◊ï◊©◊ê ◊°◊ô◊§◊®◊ô◊™ ◊°◊ô◊ß◊ô◊ò ◊ú◊®◊ü ◊î◊ô◊†◊î ◊°◊§◊®◊ô◊î ◊©◊û◊ë◊¶◊¢◊™ ◊ú◊ô◊û◊ï◊ì ◊û◊õ◊ï◊†◊î\n","permalink":"https://omers.github.io/docs/ai/slearn.he/","summary":"◊†◊ï◊©◊ê ◊°◊ô◊§◊®◊ô◊™ ◊°◊ô◊ß◊ô◊ò ◊ú◊®◊ü ◊î◊ô◊†◊î ◊°◊§◊®◊ô◊î ◊©◊û◊ë◊¶◊¢◊™ ◊ú◊ô◊û◊ï◊ì ◊û◊õ◊ï◊†◊î","title":"◊ú◊ô◊û◊ï◊ì ◊û◊õ◊ï◊†◊î"}]