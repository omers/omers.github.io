[{"content":"It is possible to manage Consul\u0026rsquo;s key-value store using version control tools like Git. One way to do this is to use a tool that synchronizes the key-value store with a Git repository. This allows you to track changes to the key-value store using Git and merge changes from multiple users using standard Git workflows.\nHere are the general steps for managing Consul\u0026rsquo;s key-value store using Git:\nInstall a tool that can synchronize the key-value store with a Git repository. There are several tools available for this purpose, such as consul-kv-sync and consul-sync.\nConfigure the synchronization tool to connect to your Consul server and the Git repository where you want to store the key-value data.\nUse the synchronization tool to export the key-value data from Consul to the Git repository. This will create a commit in the repository with the key-value data as a file.\nMake any changes to the key-value data that you want to track using Git. These changes can be made directly in the Git repository or through the Consul key-value store using the synchronization tool.\nUse the synchronization tool to import the changes from the Git repository back into Consul. This will update the key-value store with the changes made in Git.\nBy using a tool to synchronize Consul\u0026rsquo;s key-value store with a Git repository, you can track changes to the key-value store using Git and use standard Git workflows to merge changes from multiple users.\n","permalink":"https://omers.github.io/docs/cloud-management/manage-consul-kv-with-git/","summary":"It is possible to manage Consul\u0026rsquo;s key-value store using version control tools like Git. One way to do this is to use a tool that synchronizes the key-value store with a Git repository. This allows you to track changes to the key-value store using Git and merge changes from multiple users using standard Git workflows.\nHere are the general steps for managing Consul\u0026rsquo;s key-value store using Git:\nInstall a tool that can synchronize the key-value store with a Git repository.","title":"Manage Consul K/V With Git"},{"content":"Create ECR Repository with vulnerability scanning aws ecr create-repository --repository-name \u0026lt;MY_ECR_REPO_NAME\u0026gt; \\ --image-scanning-configuration scanOnPush=true --region us-east-2 ","permalink":"https://omers.github.io/docs/aws/setting-up-ecr/","summary":"Create ECR Repository with vulnerability scanning aws ecr create-repository --repository-name \u0026lt;MY_ECR_REPO_NAME\u0026gt; \\ --image-scanning-configuration scanOnPush=true --region us-east-2 ","title":"Setting Up AWS ECR service"},{"content":"What is DORA? DORA is the acronym for the DevOps Research and Assessment group: theyâ€™ve surveyed more than 50,000 technical professionals worldwide to better understand how the technical practices, cultural norms, and management approach affect organizational performance.\nThe four key metrics we measure Deployment Frequency â€” How often an organization successfully releases to production Lead Time for Changes â€” The amount of time it takes a commit to get into production Change Failure Rate â€” The percentage of deployments causing a failure in production Time to Restore Service â€” How long it takes an organization to recover from a failure in production How do we understand the metrics? Deployment Frequency - limiting amount of code going to * production at once (limited batch size) Lead Time for Changes - reducing amount of blockers for developers Time to Restore Service - improving speed of incident recovery Change Failure Rate - improving quality focus Following the rules of lean: Value is only released to production, once it leaves the factory floor (production deploy frequency) Optimize Work In Progress (lead time) Invest in SRE/automation (mean time to recover) Practice kaizen (change fail rate) Have efficient knowledge sharing and work allocation processes (time to onboard) Get into business ","permalink":"https://omers.github.io/docs/career/dora-metrics/","summary":"What is DORA? DORA is the acronym for the DevOps Research and Assessment group: theyâ€™ve surveyed more than 50,000 technical professionals worldwide to better understand how the technical practices, cultural norms, and management approach affect organizational performance.\nThe four key metrics we measure Deployment Frequency â€” How often an organization successfully releases to production Lead Time for Changes â€” The amount of time it takes a commit to get into production Change Failure Rate â€” The percentage of deployments causing a failure in production Time to Restore Service â€” How long it takes an organization to recover from a failure in production How do we understand the metrics?","title":"DORA Metrics"},{"content":"About Terracognita TerraCognita allows you to migrate your current cloud infrastructure to infrastructure-as-code. It quickly and automatically creates Terraform from all of your manually-provisioned resources, allowing them to be easily replicated by you and your cloud provider.\nThe Challange For example, There is a security group that was created manually long time ago.\nNow, You need to start amanaging and audit any events and changes related to this Security gtoup.\nThe code terracognita aws -i aws_security_group \\ --aws-access-key \u0026lt;YOUR_ACCESS_KEY\u0026gt; \\ --aws-secret-access-key \u0026lt;YOUR_SECRET_ACCESS_KEY\u0026gt; \\ --tfstate terraform.tfstate --hcl sg.tf \\ --aws-default-region us-east-2 --target \u0026lt;RESOURCE_ID\u0026gt; terracognita aws \u0026ndash;target aws_security_group.sg-0cf9d7d81a21c43c3 -i aws_security_group \u0026ndash;tfstate terraform.tfstate \u0026ndash;hcl sg.tf \u0026ndash;aws-default-region us-east-2\n","permalink":"https://omers.github.io/docs/cloud-management/importing-cloud-resources-terracognita/","summary":"About Terracognita TerraCognita allows you to migrate your current cloud infrastructure to infrastructure-as-code. It quickly and automatically creates Terraform from all of your manually-provisioned resources, allowing them to be easily replicated by you and your cloud provider.\nThe Challange For example, There is a security group that was created manually long time ago.\nNow, You need to start amanaging and audit any events and changes related to this Security gtoup.","title":"Importing Cloud Resources Terracognita"},{"content":"re:invent 2022 recorded session This session provides architectural best practices, optimizations, and useful cheat codes that you can use to build secure, high-scale, and high-performance serverless applications and uses real customer scenarios to illustrate the benefits. ","permalink":"https://omers.github.io/docs/aws/best-practices-for-serverless-developers/","summary":"re:invent 2022 recorded session This session provides architectural best practices, optimizations, and useful cheat codes that you can use to build secure, high-scale, and high-performance serverless applications and uses real customer scenarios to illustrate the benefits. ","title":"Best practices for advanced serverless developers"},{"content":"Teleport has a strong RBAC system that allows to grant access to specific resources.\nIn order to setup your environment, You need to do some preperations:\nDefine what are the Groups in your organization Define which access each group needs, For example:\nDevelopers group needs access to Grafana and server Superset Decide how to label each server / resource, and assign the labels to each resource Deinfe the policy that grants the access to the resources kind: role version: v5 metadata: name: Developers description: Developers Team Role spec: allow: logins: [\u0026#39;admin\u0026#39;] node_labels: \u0026#39;type\u0026#39;: \u0026#39;Grafana\u0026#39; \u0026#39;type\u0026#39;: \u0026#39;Superset\u0026#39; ","permalink":"https://omers.github.io/docs/teleport/access-and-authorization/","summary":"Teleport has a strong RBAC system that allows to grant access to specific resources.\nIn order to setup your environment, You need to do some preperations:\nDefine what are the Groups in your organization Define which access each group needs, For example:\nDevelopers group needs access to Grafana and server Superset Decide how to label each server / resource, and assign the labels to each resource Deinfe the policy that grants the access to the resources kind: role version: v5 metadata: name: Developers description: Developers Team Role spec: allow: logins: [\u0026#39;admin\u0026#39;] node_labels: \u0026#39;type\u0026#39;: \u0026#39;Grafana\u0026#39; \u0026#39;type\u0026#39;: \u0026#39;Superset\u0026#39; ","title":"Teleport Access and Authorization"},{"content":"Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, reduced cost, and it drives innovative solutions within the healthcare sector.\nHowever, health data is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR), which aims to ensure patient\u0026rsquo;s privacy.\nAnonymization or removal of patient identifiable information, though the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns.\nIn this post, I\u0026rsquo;ll review a Python package created my Microsoft that helps to ensure sensitive data is properly managed and governed.\nIt provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more..\nAllow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions. Embrace extensibility and customizability to a specific business need. Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms. Install pip install presidio-analyzer pip install presidio-anonymizer python -m spacy download en_core_web_lg Sample Code from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine text=\u0026#34;\u0026#34;\u0026#34; PATIENT: JOHN SMITH DOB: 5/5/1955 FILE #: 12345 PHYSICIAN: REFERRING EXAM: MRI ABDOMEN WITH CONTRAST DATE: 1/1/2011 Evaluation of the neck reveals a somewhat heterogeneous but well defined lobulated mass within the superficial lobe of the anterior right parotid gland. The mass demonstrates a rounded focus of signal prolongation with enhancement. The mass measures approximately 0.8 x 0.9 x 0.7 CM (anterior-posterior by transverse by superiorinferior). This appears similar to that noted on the prior CT. The mass just abuts the retromandibular vein, patent and medial to the right parotid duct. The mass is most consistent with a benign pleomorphic adenoma. No leftsided parotid mass is seen. Right and left submandibular glands are unremarkable. The mucosal surfaces of the upper aerodigestive tract appear symmetric and unremarkable. The larynx is intact. The nasopharynx is symmetric without distinct lesion. The tongue and tongue base appear symmetric and unremarkable. The median raphe is midline. The thyroid gland appears symmetric without distinct nodule. No pathologically enlarged lymph nodes are found. The visualized lymph nodes demonstrate no central necrosis or extranodal extension. The right and left faucial tonsil is symmetric and unremarkable. The lingual tonsillar tissue appears symmetric and of normal volume. The posterior nasopharyngeal lymphoid tissue does not appear enlarged. Evaluation of the paranasal sinuses reveals no significant sinus inflammatory disease. No air-fluid levels are noted. The central skull base is intact. The central petrous temporal bones and mastoids remain clear. The visualized base of brain appears unremarkable. Cervical spondylosis is noted, most notable for a broad-based disc bulge and dorsal osteophytic ridge at the C5/6 level with a C6/7 level focal 2 mm central disc protrusion and dorsal osteophytic ridging, resulting in mild central spinal stenosis. Mild foraminal narrowing also evident bilaterally. \u0026#34;\u0026#34;\u0026#34; # Set up the engine, loads the NLP module (spaCy model by default) # and other PII recognizers analyzer = AnalyzerEngine() # Call analyzer to get results results = analyzer.analyze(text=text,language=\u0026#39;en\u0026#39;) print(results) # Analyzer results are passed to the AnonymizerEngine for anonymization anonymizer = AnonymizerEngine() anonymized_text = anonymizer.anonymize(text=text,analyzer_results=results) print(anonymized_text) The output text: PATIENT: \u0026lt;PERSON\u0026gt;: \u0026lt;DATE_TIME\u0026gt; FILE #: 12345 PHYSICIAN: REFERRING EXAM: MRI ABDOMEN WITH CONTRAST DATE: \u0026lt;DATE_TIME\u0026gt; Marked hydronephrosis and hydroureter are present in the right kidney (series 12 images \u0026lt;DATE_TIME\u0026gt;). Low signal intensity foci in the proximal right ureter (series 6 image 36) likely represents flow related artifact. Possible septations may be present in the distal right ureter (series 12 image 20). CT scan of the abdomen and pelvis with and without contrast is recommended to evaluate for possible stone or distal obstructing lesion. Findings are new since the previous examination. Decreased enhancement of the right kidney in comparison to the left during the arterial phase (series 15 image 35) may reflect a renal compromise. Stable mild \u0026lt;PERSON\u0026gt; is again noted in the left kidney. No mass is identified in the kidneys. No masses seen along the right ureter. Postoperative changes are seen from a distal pancreatectomy and cholecystectomy representing previous \u0026lt;PERSON\u0026gt; procedure. There is dilatation of the pancreatic duct in the body and tail (series 6 images 23-20). No recurrent mass is seen in the pancreas or anastomosis. There is mild prominence of the biliary ducts in the left hepatic lobe (series 7 image 20). No filling defect is seen within the common duct. \u0026lt;PERSON\u0026gt; and adrenal glands are unremarkable. No free fluid or lymphadenopathy seen. No bowel obstruction is identified. Anterior abdominal hernia is again noted containing small bowel without evidence of strangulation (series 7 image 33). There is marked S-shaped scoliosis of the thoracolumbar spine. No metastatic bone lesions are identified. items: [ {\u0026#39;start\u0026#39;: 1296, \u0026#39;end\u0026#39;: 1304, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 987, \u0026#39;end\u0026#39;: 995, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 764, \u0026#39;end\u0026#39;: 772, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 206, \u0026#39;end\u0026#39;: 217, \u0026#39;entity_type\u0026#39;: \u0026#39;DATE_TIME\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;DATE_TIME\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 105, \u0026#39;end\u0026#39;: 116, \u0026#39;entity_type\u0026#39;: \u0026#39;DATE_TIME\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;DATE_TIME\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 20, \u0026#39;end\u0026#39;: 31, \u0026#39;entity_type\u0026#39;: \u0026#39;DATE_TIME\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;DATE_TIME\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;}, {\u0026#39;start\u0026#39;: 10, \u0026#39;end\u0026#39;: 18, \u0026#39;entity_type\u0026#39;: \u0026#39;PERSON\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;\u0026lt;PERSON\u0026gt;\u0026#39;, \u0026#39;operator\u0026#39;: \u0026#39;replace\u0026#39;} ] ","permalink":"https://omers.github.io/docs/data/anonymization/","summary":"Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, reduced cost, and it drives innovative solutions within the healthcare sector.\nHowever, health data is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR), which aims to ensure patient\u0026rsquo;s privacy.\nAnonymization or removal of patient identifiable information, though the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns.","title":"Data Protection and Anonymization"},{"content":"","permalink":"https://omers.github.io/docs/cloud-management/cloud-resources-governance/","summary":"","title":"Cloud Resources Governance"},{"content":"During the lifecycle of a company, we are required to have an assets inventory.\nThe process of having an asset inventory contains few steps.\nCloudQuery As the tool that scrapes AWS and gather all the information, We will use CloudQuery\nTimescaleDB Apache Superset ","permalink":"https://omers.github.io/docs/cloud-management/cloud-resources-dashboards/","summary":"During the lifecycle of a company, we are required to have an assets inventory.\nThe process of having an asset inventory contains few steps.\nCloudQuery As the tool that scrapes AWS and gather all the information, We will use CloudQuery\nTimescaleDB Apache Superset ","title":"Cloud Resources Dashboards"},{"content":"","permalink":"https://omers.github.io/docs/monitoring/cloudquery/","summary":"","title":"Cloudquery"},{"content":"n8n is an open-source workflow automation tool that allows you to automate tasks across different applications and services. It provides a visual interface that allows you to create workflows by connecting different nodes to perform specific tasks. n8n is built on top of Node-RED, a popular open-source programming tool for wiring together hardware devices, APIs, and online services. It is designed to be easy to use and suitable for a wide range of use cases, including integration with CRM systems, data analysis, and automation of social media tasks.\n","permalink":"https://omers.github.io/docs/automation/n8n/","summary":"n8n is an open-source workflow automation tool that allows you to automate tasks across different applications and services. It provides a visual interface that allows you to create workflows by connecting different nodes to perform specific tasks. n8n is built on top of Node-RED, a popular open-source programming tool for wiring together hardware devices, APIs, and online services. It is designed to be easy to use and suitable for a wide range of use cases, including integration with CRM systems, data analysis, and automation of social media tasks.","title":"N8n"},{"content":"This is a one liner on how to create SSL Self signed certificate with OpenSSL\nopenssl req -new -newkey rsa:4096 -days 365 \\ -nodes -x509 -keyout server.key -out server.crt ","permalink":"https://omers.github.io/docs/automation/generate-self-signed-cert/","summary":"This is a one liner on how to create SSL Self signed certificate with OpenSSL\nopenssl req -new -newkey rsa:4096 -days 365 \\ -nodes -x509 -keyout server.key -out server.crt ","title":"OpenSSL Generate self signed certificate"},{"content":"Loki is a datastore optimized for efficiently holding log data.\nThe efficient indexing of log data distinguishes Loki from other logging systems. Unlike other logging systems, a Loki index is built from labels, leaving the original log message unindexed.\nThe Basic native tool to send data to Loki, is Promtail\nA sample config file for Loki might seen like the follows:\nserver: http_listen_port: 0 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: https://your.loki.cluster scrape_configs: - job_name: system static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/*.log - targets: - localhost labels: job: syslog __path__: /var/log/syslog ","permalink":"https://omers.github.io/docs/monitoring/loki/","summary":"Loki is a datastore optimized for efficiently holding log data.\nThe efficient indexing of log data distinguishes Loki from other logging systems. Unlike other logging systems, a Loki index is built from labels, leaving the original log message unindexed.\nThe Basic native tool to send data to Loki, is Promtail\nA sample config file for Loki might seen like the follows:\nserver: http_listen_port: 0 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: https://your.","title":"Loki Log based metrics a.k.a logs2metrics"},{"content":"Quantiles from histograms 90th percentile request latency over last 5 minutes, for every label dimension:\nhistogram_quantile(0.9, rate(api_duration_seconds_bucket[5m])) Rates of increase for counters Per-second rate of increase, averaged over last 5 minutes: rate(demo_api_request_duration_seconds_count[5m]) Per-second rate of increase, calculated over last two samples in a 1-minute time window irate(demo_api_request_duration_seconds_count[1m]) Absolute increase over last hour: increase(demo_api_request_duration_seconds_count[1h]) Aggregating over time Average within each series over a 5-minute period avg_over_time(go_goroutines[5m]) Get the maximum for each series over a one-day period max_over_time(process_resident_memory_bytes[1d]) Count the number of samples for each series over a 5-minute period count_over_time(process_resident_memory_bytes[5m]) ","permalink":"https://omers.github.io/cheatsheet/prometheus/common-queries/","summary":"Quantiles from histograms 90th percentile request latency over last 5 minutes, for every label dimension:\nhistogram_quantile(0.9, rate(api_duration_seconds_bucket[5m])) Rates of increase for counters Per-second rate of increase, averaged over last 5 minutes: rate(demo_api_request_duration_seconds_count[5m]) Per-second rate of increase, calculated over last two samples in a 1-minute time window irate(demo_api_request_duration_seconds_count[1m]) Absolute increase over last hour: increase(demo_api_request_duration_seconds_count[1h]) Aggregating over time Average within each series over a 5-minute period avg_over_time(go_goroutines[5m]) Get the maximum for each series over a one-day period max_over_time(process_resident_memory_bytes[1d]) Count the number of samples for each series over a 5-minute period count_over_time(process_resident_memory_bytes[5m]) ","title":"Prometheus Common Queries"},{"content":"YOLOv5 ðŸš€ is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.\nThe project source code can be found Here\n","permalink":"https://omers.github.io/docs/ai/yolov/","summary":"YOLOv5 ðŸš€ is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.\nThe project source code can be found Here","title":"YOLO â€” You only look once"},{"content":"Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP.\nA lot of the data that you could be analyzing is unstructured data and contains human-readable text. Before you can analyze that data programmatically, you first need to preprocess it. In this tutorial, youâ€™ll take your first look at the kinds of text preprocessing tasks you can do with NLTK so that youâ€™ll be ready to apply them in future projects. Youâ€™ll also see how to do some basic text analysis and create visualizations.\nIf youâ€™re familiar with the basics of using Python and would like to get your feet wet with some NLP, then youâ€™ve come to the right place.\nBy the end of this tutorial, youâ€™ll know how to:\nFind text to analyze Preprocess your text for analysis Analyze your text Create visualizations based on your analysis Letâ€™s get Pythoning! ","permalink":"https://omers.github.io/docs/ai/python-nlp-libraries/","summary":"Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP.\nA lot of the data that you could be analyzing is unstructured data and contains human-readable text. Before you can analyze that data programmatically, you first need to preprocess it. In this tutorial, youâ€™ll take your first look at the kinds of text preprocessing tasks you can do with NLTK so that youâ€™ll be ready to apply them in future projects.","title":"Python Nlp Libraries"},{"content":"Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.\n","permalink":"https://omers.github.io/docs/ai/slearn/","summary":"Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.","title":"Python Scikit Learn"}]